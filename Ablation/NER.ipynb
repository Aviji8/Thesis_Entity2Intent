{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 308/308 [04:51<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Results saved to b77_zs_ablition_flat_entity_gpt.json\n",
      "\n",
      "üìä Accuracy: 45.78%\n",
      "üîç Mistakes saved: 167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "from collections import Counter\n",
    "\n",
    "# Set OpenAI API Key\n",
    "openai.api_key =  \"\"\n",
    "\n",
    "# 1. Prompt Builder: Entity Extraction ‚Üí Intent Reasoning\n",
    "def build_prompt(user_input: str, topn_labels: List[str]) -> str:\n",
    "    formatted_options = [label.replace(\"_\", \" \") for label in topn_labels]\n",
    "    options_block = \"\\n\".join(f\"- {opt}\" for opt in formatted_options)\n",
    "\n",
    "    prompt = f\"\"\"Extract the key terms or entities in the sentence below that are important for intent understanding. \n",
    "Do not assign specific roles‚Äîjust list the important words or phrases.\n",
    "\n",
    "Then, based on these key terms, reason about which intent best matches the user query.\n",
    "\n",
    "OPTIONS:\n",
    "{options_block}\n",
    "\n",
    "QUERY: {user_input}\n",
    "\n",
    "Respond in the following format:\n",
    "ENTITIES: ...\n",
    "EXPLANATION: ...\n",
    "LABEL: ... (must be in snake_case)\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# 2. Query OpenAI GPT model\n",
    "def query_model(prompt: str) -> str:\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant for understanding and classifying intent.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# 3. Parse the GPT response\n",
    "def parse_response(response: str) -> Dict[str, str]:\n",
    "    try:\n",
    "        entities = re.search(r\"ENTITIES:\\s*(.*?)(?:\\n|EXPLANATION:)\", response, re.IGNORECASE | re.DOTALL)\n",
    "        explanation = re.search(r\"EXPLANATION:\\s*(.*?)(?:\\n|LABEL:)\", response, re.IGNORECASE | re.DOTALL)\n",
    "        label = re.search(r\"LABEL:\\s*(.+)\", response, re.IGNORECASE)\n",
    "\n",
    "        return {\n",
    "            \"entities\": entities.group(1).strip() if entities else \"N/A\",\n",
    "            \"explanation\": explanation.group(1).strip() if explanation else \"N/A\",\n",
    "            \"predicted_label\": label.group(1).strip() if label else \"Unknown\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Parsing error:\", e)\n",
    "        return {\"entities\": \"N/A\", \"explanation\": \"N/A\", \"predicted_label\": \"Unknown\"}\n",
    "\n",
    "# 4. Full Pipeline\n",
    "def run_flat_entity_ablation(input_csv: str, output_json: str):\n",
    "    df = pd.read_csv(input_csv)#.iloc[:10]\n",
    "    results = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        text = row[\"text\"]\n",
    "        true_label = row[\"label\"]\n",
    "        top_k_predictions = eval(row[\"top_k_predictions\"])  # Convert string to list\n",
    "\n",
    "        prompt = build_prompt(text, top_k_predictions)\n",
    "        response = query_model(prompt)\n",
    "        parsed = parse_response(response)\n",
    "\n",
    "        results.append({\n",
    "            \"id\": int(row[\"id\"]),\n",
    "            \"text\": text,\n",
    "            \"true_label\": true_label,\n",
    "            \"top_k_predictions\": top_k_predictions,\n",
    "            \"entities\": parsed[\"entities\"],\n",
    "            \"explanation\": parsed[\"explanation\"],\n",
    "            \"predicted_label\": parsed[\"predicted_label\"]\n",
    "        })\n",
    "\n",
    "    # Save results\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Results saved to {output_json}\")\n",
    "    return results\n",
    "\n",
    "# 5. Accuracy Evaluation\n",
    "def evaluate_accuracy(results: List[Dict]):\n",
    "    total = len(results)\n",
    "    correct = sum(1 for r in results if r[\"true_label\"].strip() == r[\"predicted_label\"].strip())\n",
    "    accuracy = correct / total if total else 0\n",
    "    print(\"\\nüìä Accuracy:\", f\"{accuracy*100:.2f}%\")\n",
    "\n",
    "    mistakes = [r for r in results if r[\"true_label\"].strip() != r[\"predicted_label\"].strip()]\n",
    "    if mistakes:\n",
    "        with open(\"flat_entity_ablation_mistakes.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(mistakes, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"üîç Mistakes saved: {len(mistakes)}\")\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# 6. Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = \"/home/xrspace/Llama/Thesis/E2i/maccot_topk_formatted.csv\"  # <--- Replace with your file path\n",
    "    output_json = \"b77_zs_ablition_flat_entity_gpt.json\"\n",
    "    \n",
    "    results = run_flat_entity_ablation(input_csv, output_json)\n",
    "    evaluate_accuracy(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fs b77 flat ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/308 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 308/308 [15:34<00:00,  3.04s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Results saved to b77_fs_ablition_predict_flat_entity_gpt.json\n",
      "üîç Mistakes saved to: b77_fs_ablition_mistake_flat_entity_gpt.json\n",
      "üìä Accuracy: 51.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load few-shot support file (flat entity version)\n",
    "with open(\"/home/xrspace/Llama/Thesis/E2i/ablition/ner_flat_b77_fs_sample.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    FEWSHOT_DB = json.load(f)\n",
    "\n",
    "# Organize few-shot examples by label\n",
    "label_to_example = defaultdict(list)\n",
    "for entry in FEWSHOT_DB:\n",
    "    label_to_example[entry[\"true_label\"]].append(entry)\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = \"key\"  # Replace with your actual key\n",
    "\n",
    "# Build prompt using dynamic few-shot samples (flat entity style)\n",
    "def build_prompt(user_input: str, topn_labels: List[str]) -> str:\n",
    "    formatted_options = [label.replace(\"_\", \" \") for label in topn_labels]\n",
    "    options_block = \"\\n\".join(f\"- {opt}\" for opt in formatted_options)\n",
    "\n",
    "    few_shots = []\n",
    "    for label in topn_labels:\n",
    "        if label in label_to_example:\n",
    "            ex = label_to_example[label][0]\n",
    "            few_shots.append(f\"\"\"Example:\n",
    "Query: \"{ex['text']}\"\n",
    "ENTITIES: {ex['entities']}\n",
    "EXPLANATION: {ex['explanation']}\n",
    "LABEL: {label}\n",
    "---\"\"\")\n",
    "\n",
    "    few_shot_block = \"\\n\".join(few_shots)\n",
    "\n",
    "    return f\"\"\"Extract the key terms or entities in the sentence below that are important for intent understanding. \n",
    "Do not assign specific roles‚Äîjust list the important words or phrases.\n",
    "\n",
    "Then, based on these key terms, reason about which intent best matches the user query.\n",
    "\n",
    "{few_shot_block}\n",
    "\n",
    "OPTIONS:\n",
    "{options_block}\n",
    "\n",
    "QUERY: {user_input}\n",
    "\n",
    "Respond in the following format:\n",
    "ENTITIES: ...\n",
    "EXPLANATION: ...\n",
    "LABEL: ... (must be in snake_case)\n",
    "\"\"\"\n",
    "\n",
    "# Parse response\n",
    "def parse_response(response: str) -> Dict[str, str]:\n",
    "    try:\n",
    "        entities = re.search(r\"ENTITIES:\\s*(.*?)(?:\\n|EXPLANATION:)\", response, re.IGNORECASE | re.DOTALL)\n",
    "        explanation = re.search(r\"EXPLANATION:\\s*(.*?)(?:\\n|LABEL:)\", response, re.IGNORECASE | re.DOTALL)\n",
    "        label = re.search(r\"LABEL:\\s*(.+)\", response, re.IGNORECASE)\n",
    "        return {\n",
    "            \"entities\": entities.group(1).strip() if entities else \"N/A\",\n",
    "            \"explanation\": explanation.group(1).strip() if explanation else \"N/A\",\n",
    "            \"predicted_label\": label.group(1).strip() if label else \"Unknown\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(\"Parsing error:\", e)\n",
    "        return {\"entities\": \"N/A\", \"explanation\": \"N/A\", \"predicted_label\": \"Unknown\"}\n",
    "\n",
    "# Query GPT model\n",
    "def query_model(prompt: str) -> str:\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant for intent classification.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=600\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Run classification\n",
    "def run_experiment(input_csv: str, output_json: str) -> List[Dict]:\n",
    "    df = pd.read_csv(input_csv)#.iloc[:10]\n",
    "    results = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        text = row[\"text\"]\n",
    "        true_label = row[\"label\"]\n",
    "        topk_predictions = eval(row[\"top_k_predictions\"])\n",
    "        prompt = build_prompt(text, topk_predictions)\n",
    "        response = query_model(prompt)\n",
    "        parsed = parse_response(response)\n",
    "        results.append({\n",
    "            \"id\": int(row[\"id\"]),\n",
    "            \"text\": text,\n",
    "            \"true_label\": true_label,\n",
    "            \"top_k_predictions\": topk_predictions,\n",
    "            \"entities\": parsed[\"entities\"],\n",
    "            \"explanation\": parsed[\"explanation\"],\n",
    "            \"predicted_label\": parsed[\"predicted_label\"]\n",
    "        })\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Results saved to {output_json}\")\n",
    "    return results\n",
    "\n",
    "# Accuracy check\n",
    "def evaluate_accuracy(results: List[Dict], mistake_file: str = \"b77_fs_ablition_mistake_flat_entity_gpt.json\"):\n",
    "    total = len(results)\n",
    "    correct = sum(1 for r in results if r[\"true_label\"].strip() == r[\"predicted_label\"].strip())\n",
    "    accuracy = correct / total if total else 0\n",
    "    mistakes = [r for r in results if r[\"true_label\"].strip() != r[\"predicted_label\"].strip()]\n",
    "    if mistakes:\n",
    "        with open(mistake_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(mistakes, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"üîç Mistakes saved to: {mistake_file}\")\n",
    "    print(f\"üìä Accuracy: {accuracy*100:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = \"/home/xrspace/Llama/Thesis/E2i/maccot_topk_formatted.csv\"  # Replace with your CSV path\n",
    "    output_json = \"b77_fs_ablition_predict_flat_entity_gpt.json\"\n",
    "    mistake_file = \"b77_fs_ablition_mistake_flat_entity_gpt.json\"\n",
    "    results = run_experiment(input_csv, output_json)\n",
    "    evaluate_accuracy(results, mistake_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "liu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 254/254 [03:36<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Results saved to liu_fs_ablition_predict_flat_entity_gpt.json\n",
      "üîç Mistakes saved to: liu_fs_ablition_mistake_flat_entity_gpt.json\n",
      "üìä Accuracy: 41.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load few-shot support file (flat entity version)\n",
    "with open(\"/home/xrspace/Llama/Thesis/E2i/ablition/liu_flat_fs_enttiy_sample.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    FEWSHOT_DB = json.load(f)\n",
    "\n",
    "# Organize few-shot examples by label\n",
    "label_to_example = defaultdict(list)\n",
    "for entry in FEWSHOT_DB:\n",
    "    label_to_example[entry[\"true_label\"]].append(entry)\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = \"key\"  # Replace with your actual key\n",
    "\n",
    "# Build prompt using dynamic few-shot samples (flat entity style)\n",
    "def build_prompt(user_input: str, topn_labels: List[str]) -> str:\n",
    "    formatted_options = [label.replace(\"_\", \" \") for label in topn_labels]\n",
    "    options_block = \"\\n\".join(f\"- {opt}\" for opt in formatted_options)\n",
    "\n",
    "    few_shots = []\n",
    "    for label in topn_labels:\n",
    "        if label in label_to_example:\n",
    "            ex = label_to_example[label][0]\n",
    "            few_shots.append(f\"\"\"Example:\n",
    "Query: \"{ex['text']}\"\n",
    "ENTITIES: {ex['entities']}\n",
    "EXPLANATION: {ex['explanation']}\n",
    "LABEL: {label}\n",
    "---\"\"\")\n",
    "\n",
    "    few_shot_block = \"\\n\".join(few_shots)\n",
    "\n",
    "    return f\"\"\"Extract the key terms or entities in the sentence below that are important for intent understanding. \n",
    "Do not assign specific roles‚Äîjust list the important words or phrases.\n",
    "\n",
    "Then, based on these key terms, reason about which intent best matches the user query.\n",
    "\n",
    "{few_shot_block}\n",
    "\n",
    "OPTIONS:\n",
    "{options_block}\n",
    "\n",
    "QUERY: {user_input}\n",
    "\n",
    "Respond in the following format:\n",
    "ENTITIES: ...\n",
    "EXPLANATION: ...\n",
    "LABEL: ... (must be in snake_case)\n",
    "\"\"\"\n",
    "\n",
    "# Parse response\n",
    "def parse_response(response: str) -> Dict[str, str]:\n",
    "    try:\n",
    "        entities = re.search(r\"ENTITIES:\\s*(.*?)(?:\\n|EXPLANATION:)\", response, re.IGNORECASE | re.DOTALL)\n",
    "        explanation = re.search(r\"EXPLANATION:\\s*(.*?)(?:\\n|LABEL:)\", response, re.IGNORECASE | re.DOTALL)\n",
    "        label = re.search(r\"LABEL:\\s*(.+)\", response, re.IGNORECASE)\n",
    "        return {\n",
    "            \"entities\": entities.group(1).strip() if entities else \"N/A\",\n",
    "            \"explanation\": explanation.group(1).strip() if explanation else \"N/A\",\n",
    "            \"predicted_label\": label.group(1).strip() if label else \"Unknown\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(\"Parsing error:\", e)\n",
    "        return {\"entities\": \"N/A\", \"explanation\": \"N/A\", \"predicted_label\": \"Unknown\"}\n",
    "\n",
    "# Query GPT model\n",
    "def query_model(prompt: str) -> str:\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant for intent classification.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=600\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Run classification\n",
    "def run_experiment(input_csv: str, output_json: str) -> List[Dict]:\n",
    "    df = pd.read_csv(input_csv)#.iloc[:10]\n",
    "    results = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        text = row[\"text\"]\n",
    "        true_label = row[\"label\"]\n",
    "        topk_predictions = eval(row[\"top_k_predictions\"])\n",
    "        prompt = build_prompt(text, topk_predictions)\n",
    "        response = query_model(prompt)\n",
    "        parsed = parse_response(response)\n",
    "        results.append({\n",
    "            \"id\": int(row[\"id\"]),\n",
    "            \"text\": text,\n",
    "            \"true_label\": true_label,\n",
    "            \"top_k_predictions\": topk_predictions,\n",
    "            \"entities\": parsed[\"entities\"],\n",
    "            \"explanation\": parsed[\"explanation\"],\n",
    "            \"predicted_label\": parsed[\"predicted_label\"]\n",
    "        })\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Results saved to {output_json}\")\n",
    "    return results\n",
    "\n",
    "# Accuracy check\n",
    "def evaluate_accuracy(results: List[Dict], mistake_file: str = \"liu_fs_ablition_mistake_flat_entity_gpt.json\"):\n",
    "    total = len(results)\n",
    "    correct = sum(1 for r in results if r[\"true_label\"].strip() == r[\"predicted_label\"].strip())\n",
    "    accuracy = correct / total if total else 0\n",
    "    mistakes = [r for r in results if r[\"true_label\"].strip() != r[\"predicted_label\"].strip()]\n",
    "    if mistakes:\n",
    "        with open(mistake_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(mistakes, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"üîç Mistakes saved to: {mistake_file}\")\n",
    "    print(f\"üìä Accuracy: {accuracy*100:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = \"/home/xrspace/Llama/Thesis/E2i/maccot_topk_formatted_liu.csv\"  # Replace with your CSV path\n",
    "    output_json = \"liu_fs_ablition_predict_flat_entity_gpt.json\"\n",
    "    mistake_file = \"liu_fs_ablition_mistake_flat_entity_gpt.json\"\n",
    "    results = run_experiment(input_csv, output_json)\n",
    "    evaluate_accuracy(results, mistake_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clinc zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 225/225 [03:21<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Results saved to clinc_zs_ablition_flat_entity_prediction_gpt.json\n",
      "\n",
      "üìä Accuracy: 72.00%\n",
      "üîç Mistakes saved: 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "from collections import Counter\n",
    "\n",
    "# Set OpenAI API Key\n",
    "openai.api_key =  \"key\"\n",
    "\n",
    "# 1. Prompt Builder: Entity Extraction ‚Üí Intent Reasoning\n",
    "def build_prompt(user_input: str, topn_labels: List[str]) -> str:\n",
    "    formatted_options = [label.replace(\"_\", \" \") for label in topn_labels]\n",
    "    options_block = \"\\n\".join(f\"- {opt}\" for opt in formatted_options)\n",
    "\n",
    "    prompt = f\"\"\"Extract the key terms or entities in the sentence below that are important for intent understanding. \n",
    "Do not assign specific roles‚Äîjust list the important words or phrases.\n",
    "\n",
    "Then, based on these key terms, reason about which intent best matches the user query.\n",
    "\n",
    "OPTIONS:\n",
    "{options_block}\n",
    "\n",
    "QUERY: {user_input}\n",
    "\n",
    "Respond in the following format:\n",
    "ENTITIES: ...\n",
    "EXPLANATION: ...\n",
    "LABEL: ... (must be in snake_case)\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# 2. Query OpenAI GPT model\n",
    "def query_model(prompt: str) -> str:\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant for understanding and classifying intent.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# 3. Parse the GPT response\n",
    "def parse_response(response: str) -> Dict[str, str]:\n",
    "    try:\n",
    "        entities = re.search(r\"ENTITIES:\\s*(.*?)(?:\\n|EXPLANATION:)\", response, re.IGNORECASE | re.DOTALL)\n",
    "        explanation = re.search(r\"EXPLANATION:\\s*(.*?)(?:\\n|LABEL:)\", response, re.IGNORECASE | re.DOTALL)\n",
    "        label = re.search(r\"LABEL:\\s*(.+)\", response, re.IGNORECASE)\n",
    "\n",
    "        return {\n",
    "            \"entities\": entities.group(1).strip() if entities else \"N/A\",\n",
    "            \"explanation\": explanation.group(1).strip() if explanation else \"N/A\",\n",
    "            \"predicted_label\": label.group(1).strip() if label else \"Unknown\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Parsing error:\", e)\n",
    "        return {\"entities\": \"N/A\", \"explanation\": \"N/A\", \"predicted_label\": \"Unknown\"}\n",
    "\n",
    "# 4. Full Pipeline\n",
    "def run_flat_entity_ablation(input_csv: str, output_json: str):\n",
    "    df = pd.read_csv(input_csv)#.iloc[:10]\n",
    "    results = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        text = row[\"text\"]\n",
    "        true_label = row[\"label\"]\n",
    "        top_k_predictions = eval(row[\"top_k_predictions\"])  # Convert string to list\n",
    "\n",
    "        prompt = build_prompt(text, top_k_predictions)\n",
    "        response = query_model(prompt)\n",
    "        parsed = parse_response(response)\n",
    "\n",
    "        results.append({\n",
    "            \"id\": int(row[\"id\"]),\n",
    "            \"text\": text,\n",
    "            \"true_label\": true_label,\n",
    "            \"top_k_predictions\": top_k_predictions,\n",
    "            \"entities\": parsed[\"entities\"],\n",
    "            \"explanation\": parsed[\"explanation\"],\n",
    "            \"predicted_label\": parsed[\"predicted_label\"]\n",
    "        })\n",
    "\n",
    "    # Save results\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Results saved to {output_json}\")\n",
    "    return results\n",
    "\n",
    "# 5. Accuracy Evaluation\n",
    "def evaluate_accuracy(results: List[Dict]):\n",
    "    total = len(results)\n",
    "    correct = sum(1 for r in results if r[\"true_label\"].strip() == r[\"predicted_label\"].strip())\n",
    "    accuracy = correct / total if total else 0\n",
    "    print(\"\\nüìä Accuracy:\", f\"{accuracy*100:.2f}%\")\n",
    "\n",
    "    mistakes = [r for r in results if r[\"true_label\"].strip() != r[\"predicted_label\"].strip()]\n",
    "    if mistakes:\n",
    "        with open(\"clinc_zs_ablition_flat_entity_mistake_gpt.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(mistakes, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"üîç Mistakes saved: {len(mistakes)}\")\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# 6. Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = \"/home/xrspace/Llama/Thesis/E2i/maccot_topk_formatted_clinc.csv\"  # <--- Replace with your file path\n",
    "    output_json = \"clinc_zs_ablition_flat_entity_prediction_gpt.json\"\n",
    "    \n",
    "    results = run_flat_entity_ablation(input_csv, output_json)\n",
    "    evaluate_accuracy(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clinc fs NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 225/225 [03:12<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Results saved to clinc_fs_ablition_predict_flat_entity_gpt.json\n",
      "üîç Mistakes saved to: clinc_fs_ablition_mistake_flat_entity_gpt.json\n",
      "üìä Accuracy: 86.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load few-shot support file (flat entity version)\n",
    "with open(\"/home/xrspace/Llama/Thesis/E2i/ablition/clinic_flat_fs_enttiy_sample.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    FEWSHOT_DB = json.load(f)\n",
    "\n",
    "# Organize few-shot examples by label\n",
    "label_to_example = defaultdict(list)\n",
    "for entry in FEWSHOT_DB:\n",
    "    label_to_example[entry[\"true_label\"]].append(entry)\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = \"key\"  # Replace with your actual key\n",
    "\n",
    "# Build prompt using dynamic few-shot samples (flat entity style)\n",
    "def build_prompt(user_input: str, topn_labels: List[str]) -> str:\n",
    "    formatted_options = [label.replace(\"_\", \" \") for label in topn_labels]\n",
    "    options_block = \"\\n\".join(f\"- {opt}\" for opt in formatted_options)\n",
    "\n",
    "    few_shots = []\n",
    "    for label in topn_labels:\n",
    "        if label in label_to_example:\n",
    "            ex = label_to_example[label][0]\n",
    "            few_shots.append(f\"\"\"Example:\n",
    "Query: \"{ex['text']}\"\n",
    "ENTITIES: {ex['entities']}\n",
    "EXPLANATION: {ex['explanation']}\n",
    "LABEL: {label}\n",
    "---\"\"\")\n",
    "\n",
    "    few_shot_block = \"\\n\".join(few_shots)\n",
    "\n",
    "    return f\"\"\"Extract the key terms or entities in the sentence below that are important for intent understanding. \n",
    "Do not assign specific roles‚Äîjust list the important words or phrases.\n",
    "\n",
    "Then, based on these key terms, reason about which intent best matches the user query.\n",
    "\n",
    "{few_shot_block}\n",
    "\n",
    "OPTIONS:\n",
    "{options_block}\n",
    "\n",
    "QUERY: {user_input}\n",
    "\n",
    "Respond in the following format:\n",
    "ENTITIES: ...\n",
    "EXPLANATION: ...\n",
    "LABEL: ... (must be in snake_case)\n",
    "\"\"\"\n",
    "\n",
    "# Parse response\n",
    "def parse_response(response: str) -> Dict[str, str]:\n",
    "    try:\n",
    "        entities = re.search(r\"ENTITIES:\\s*(.*?)(?:\\n|EXPLANATION:)\", response, re.IGNORECASE | re.DOTALL)\n",
    "        explanation = re.search(r\"EXPLANATION:\\s*(.*?)(?:\\n|LABEL:)\", response, re.IGNORECASE | re.DOTALL)\n",
    "        label = re.search(r\"LABEL:\\s*(.+)\", response, re.IGNORECASE)\n",
    "        return {\n",
    "            \"entities\": entities.group(1).strip() if entities else \"N/A\",\n",
    "            \"explanation\": explanation.group(1).strip() if explanation else \"N/A\",\n",
    "            \"predicted_label\": label.group(1).strip() if label else \"Unknown\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(\"Parsing error:\", e)\n",
    "        return {\"entities\": \"N/A\", \"explanation\": \"N/A\", \"predicted_label\": \"Unknown\"}\n",
    "\n",
    "# Query GPT model\n",
    "def query_model(prompt: str) -> str:\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant for intent classification.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=600\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Run classification\n",
    "def run_experiment(input_csv: str, output_json: str) -> List[Dict]:\n",
    "    df = pd.read_csv(input_csv)#.iloc[:10]\n",
    "    results = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        text = row[\"text\"]\n",
    "        true_label = row[\"label\"]\n",
    "        topk_predictions = eval(row[\"top_k_predictions\"])\n",
    "        prompt = build_prompt(text, topk_predictions)\n",
    "        response = query_model(prompt)\n",
    "        parsed = parse_response(response)\n",
    "        results.append({\n",
    "            \"id\": int(row[\"id\"]),\n",
    "            \"text\": text,\n",
    "            \"true_label\": true_label,\n",
    "            \"top_k_predictions\": topk_predictions,\n",
    "            \"entities\": parsed[\"entities\"],\n",
    "            \"explanation\": parsed[\"explanation\"],\n",
    "            \"predicted_label\": parsed[\"predicted_label\"]\n",
    "        })\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Results saved to {output_json}\")\n",
    "    return results\n",
    "\n",
    "# Accuracy check\n",
    "def evaluate_accuracy(results: List[Dict], mistake_file: str = \"clinc_fs_ablition_mistake_flat_entity_gpt.json\"):\n",
    "    total = len(results)\n",
    "    correct = sum(1 for r in results if r[\"true_label\"].strip() == r[\"predicted_label\"].strip())\n",
    "    accuracy = correct / total if total else 0\n",
    "    mistakes = [r for r in results if r[\"true_label\"].strip() != r[\"predicted_label\"].strip()]\n",
    "    if mistakes:\n",
    "        with open(mistake_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(mistakes, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"üîç Mistakes saved to: {mistake_file}\")\n",
    "    print(f\"üìä Accuracy: {accuracy*100:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = \"/home/xrspace/Llama/Thesis/E2i/maccot_topk_formatted_clinc.csv\"  # Replace with your CSV path\n",
    "    output_json = \"clinc_fs_ablition_predict_flat_entity_gpt.json\"\n",
    "    mistake_file = \"clinc_fs_ablition_mistake_flat_entity_gpt.json\"\n",
    "    results = run_experiment(input_csv, output_json)\n",
    "    evaluate_accuracy(results, mistake_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-xut4REbk-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
